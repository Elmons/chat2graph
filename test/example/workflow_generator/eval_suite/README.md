# YAML Evaluation Suite

This folder is method-agnostic and is used to evaluate YAMLs generated by different methods.

## Folder layout

- `yamls/`: put candidate YAML files here.
  - Example: `yamls/[codex]_graph_query_single_expert_builtin.yml`
- `eval_yaml_method_benchmark.py`: no-arg single-YAML eval script.
- `eval_runs/`: run outputs (`overview.json`, `results.json`, `summary.json`, `run_meta.json`).

## Run

```bash
python test/example/workflow_generator/eval_suite/eval_yaml_method_benchmark.py
```

## Configure one run

Edit constants in `eval_yaml_method_benchmark.py`:

- `YAML_PATH`: YAML file to evaluate.
- `DATASET_PATH`: dataset JSON path.
- `TASK_DESC`: task description used when loading dataset.
- `MAIN_EXPERT_NAME`: entry expert name (single-expert YAML usually uses `"Main Expert"`).
- `SCORE_MODE`: `"llm"` or `"exact"` (current default is `"llm"`).

Example:

```python
TASK_DESC = "你的主要职责是解决关于图数据库的各种问题，包括实体查询、多跳推理等等"
DATASET_PATH = "test/example/workflow_generator/data_example.json"
YAML_PATH = "test/example/workflow_generator/eval_suite/yamls/[codex]_graph_query_single_expert_builtin.yml"
MAIN_EXPERT_NAME = "Main Expert"
SCORE_MODE = "llm"
```

## Notes

- Current `yamls/` is flat (no method subfolders required).
- Each run writes one new timestamped folder under `eval_runs/`.
- Check `overview.json` first for aggregate metrics.
